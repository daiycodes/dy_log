{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. 패키지 불러오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "table = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반 영상\n",
    "def general_video(url):\n",
    "    global table\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # 제목\n",
    "    title = driver.find_element(By.XPATH, '//*[@id=\"watch7-content\"]/meta[1]').get_attribute('content')\n",
    "\n",
    "    # 댓글 쪽으로 스크롤하기\n",
    "    driver.execute_script(\"window.scrollTo(0, 500)\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    # 더보기 버튼 클릭\n",
    "    sort_button = driver.find_element(By.XPATH, '//*[@id=\"expand\"]')\n",
    "    sort_button.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # published 날짜\n",
    "    date = driver.find_element(By.XPATH, '//*[@id=\"info\"]/span[3]').text\n",
    "\n",
    "    # 댓글 개수\n",
    "    count_comment_element = driver.find_element(By.XPATH, '//*[@id=\"count\"]/yt-formatted-string/span[1]')\n",
    "    count_comment = count_comment_element.text\n",
    "\n",
    "    # 댓글 정렬 버튼 클릭\n",
    "    sort_button = driver.find_element(By.XPATH, '//*[@id=\"icon-label\"]')\n",
    "    sort_button.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 인기순 클릭\n",
    "    popular_option = driver.find_element(By.XPATH, '//*[@id=\"item-with-badge\"]/div')\n",
    "    popular_option.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 댓글 데이터 수집\n",
    "    comments = []\n",
    "    likes_counts = []  # 댓글 좋아요 수\n",
    "    comment_dates = []  # 댓글 작성 날짜\n",
    "    comment_sub_counts = []  # 대댓글 수 저장 리스트\n",
    "\n",
    "    # 페이지 로딩\n",
    "    for _ in range(30):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "\n",
    "    # 파싱할 데이터 불러오기\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "\n",
    "    # 댓글 정보 수집\n",
    "    comment_list = soup.select(\"ytd-comment-thread-renderer\")\n",
    "    for comment in comment_list:\n",
    "        if len(comments) < 500:\n",
    "            # 댓글 내용\n",
    "            temp_comment = comment.select_one(\"#content-text\").text.strip()\n",
    "            comments.append(temp_comment)\n",
    "\n",
    "            # 댓글 좋아요 수\n",
    "            like_count = comment.select_one(\"#vote-count-middle\").text.strip() if comment.select_one(\"#vote-count-middle\") else '0'\n",
    "            likes_counts.append(like_count)\n",
    "                    \n",
    "            # 댓글 작성 날짜\n",
    "            comment_date_element = comment.select_one(\"#published-time-text a\")\n",
    "            comment_date = comment_date_element.text.strip() if comment_date_element else 'N/A'\n",
    "            comment_dates.append(comment_date)\n",
    "\n",
    "            # 대댓글 수 추출\n",
    "            sub_comment_count_element = comment.select_one(\"button[aria-label*='reply'], button[aria-label*='replies']\")\n",
    "            if sub_comment_count_element:\n",
    "                sub_comment_count_text = sub_comment_count_element.get('aria-label', '').strip()\n",
    "                # 숫자만 추출하는 정규 표현식 사용\n",
    "                \n",
    "                match = re.search(r'\\d+', sub_comment_count_text)\n",
    "                \n",
    "                if match:\n",
    "                    comment_sub_counts.append(int(match.group(0)))  # 추출한 숫자를 정수로 변환하여 추가\n",
    "                else:\n",
    "                    comment_sub_counts.append(0)  # 숫자가 없으면 0으로\n",
    "            else:\n",
    "                comment_sub_counts.append(0)  # 대댓글이 없으면 0으로\n",
    "\n",
    "    # 영상 제목, 영상 published 날짜, 댓글 수, 인기 댓글 내용, 댓글 좋아요 수, 대댓글 수 전부 한 곳에 모으기\n",
    "    for comment in comments:\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'count_comment': count_comment,\n",
    "            'comment': comment,\n",
    "            'likes': likes_counts.pop(0) if likes_counts else '0',  # likes_counts에서 첫 번째 요소 제거\n",
    "            'comment_date': comment_dates.pop(0) if comment_dates else 'N/A',  # comment_dates에서 첫 번째 요소 제거\n",
    "            'sub_comment_count': comment_sub_counts.pop(0) if comment_sub_counts else 0  # sub_comment_counts에서 첫 번째 요소 제거\n",
    "        })\n",
    "    \n",
    "    # 데이터프레임으로 변환\n",
    "    table = table.append(pd.DataFrame(results), ignore_index=True)\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쇼츠\n",
    "def short_video(url):\n",
    "    global table\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # 제목\n",
    "    title = driver.find_element(By.XPATH, '//*[@id=\"watch7-content\"]/meta[1]').get_attribute('content')\n",
    "\n",
    "    # published 날짜 추출 개선\n",
    "    date_elements = driver.find_elements(By.XPATH, '//factoid-renderer//div[@class=\"ytwFactoidRendererFactoid\"]')\n",
    "\n",
    "    # 날짜 정보 추출\n",
    "    date = ''\n",
    "    aria_label = date_elements[2].get_attribute('aria-label')  # aria-label에서 날짜 추출\n",
    "    if aria_label:  # aria-label이 존재하는 경우\n",
    "        date = aria_label.strip()  # 날짜 값을 가져옴\n",
    "\n",
    "    # 댓글 클릭하기\n",
    "    comment_button = driver.find_element(By.XPATH, '//*[@id=\"comments-button\"]/ytd-button-renderer/yt-button-shape/label/button/yt-touch-feedback-shape/div')\n",
    "    comment_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # 댓글 개수\n",
    "    count_comment_element = driver.find_element(By.XPATH, '//*[@id=\"comments-button\"]/ytd-button-renderer/yt-button-shape/label/div/span')\n",
    "    count_comment = count_comment_element.text\n",
    "\n",
    "    # 댓글 정렬 버튼 클릭\n",
    "    sort_button = driver.find_element(By.XPATH, '//*[@id=\"label-icon\"]')\n",
    "    sort_button.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # 인기순 클릭\n",
    "    popular_option = driver.find_element(By.XPATH, '//*[@id=\"menu\"]/a[1]/tp-yt-paper-item/tp-yt-paper-item-body')\n",
    "    popular_option.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 댓글 정렬 버튼 한번 더 클릭 \n",
    "    sort_button = driver.find_element(By.XPATH, '//*[@id=\"label-icon\"]')\n",
    "    sort_button.click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # 댓글 데이터 수집\n",
    "    comments = []\n",
    "    likes_counts = []  # 댓글 좋아요 수\n",
    "    comment_dates = []  # 댓글 작성 날짜\n",
    "    comment_sub_counts = []  # 대댓글 수 저장 리스트\n",
    "\n",
    "    # 댓글 영역 선택\n",
    "    reply_body = driver.find_element(By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-shorts/div[4]/div[2]/ytd-engagement-panel-section-list-renderer[1]/div[2]/ytd-section-list-renderer/div[2]/ytd-comments/ytd-item-section-renderer/div[3]')\n",
    "     \n",
    "    #댓글 스크롤\n",
    "    for _ in range(40):\n",
    "        driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight;', reply_body)\n",
    "        time.sleep(2)  # 페이지 로딩 대기\n",
    "           \n",
    "    # 파싱할 데이터 불러오기\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "        \n",
    "    # 댓글 정보 수집\n",
    "    comment_list = soup.select(\"ytd-comment-thread-renderer\")\n",
    "    for comment in comment_list:\n",
    "        if len(comments) < 500:\n",
    "            # 댓글 내용\n",
    "            temp_comment = comment.select_one(\"#content-text\").text.strip()\n",
    "            comments.append(temp_comment)\n",
    "\n",
    "            # 댓글 좋아요 수\n",
    "            like_count = comment.select_one(\"#vote-count-middle\").text.strip() if comment.select_one(\"#vote-count-middle\") else '0'\n",
    "            likes_counts.append(like_count)\n",
    "\n",
    "            # 댓글 작성 날짜\n",
    "            comment_date_element = comment.select_one(\"#published-time-text a\")\n",
    "            comment_date = comment_date_element.text.strip() if comment_date_element else 'N/A'\n",
    "            comment_dates.append(comment_date)\n",
    "                    \n",
    "            # 대댓글 수 추출\n",
    "            sub_comment_count_element = comment.select_one(\"button[aria-label*='reply'], button[aria-label*='replies']\")\n",
    "            if sub_comment_count_element:\n",
    "                sub_comment_count_text = sub_comment_count_element.get('aria-label', '').strip()\n",
    "                comment_sub_counts.append(int(sub_comment_count_text.split(' ')[0]))\n",
    "            else:\n",
    "                comment_sub_counts.append(0)  # 대댓글이 없으면 0으로\n",
    "                \n",
    "        else:\n",
    "            break\n",
    "        \n",
    "\n",
    "    # 영상 제목, 영상 published 날짜, 댓글 수, 인기 댓글 내용, 댓글 좋아요 수, 대댓글 수 전부 한 곳에 모으기\n",
    "    for comment in comments:\n",
    "        results.append({\n",
    "            'title': title,\n",
    "            'date': date,\n",
    "            'count_comment': count_comment,\n",
    "            'comment': comment,\n",
    "            'likes': likes_counts.pop(0) if likes_counts else '0',  # likes_counts에서 첫 번째 요소 제거\n",
    "            'comment_date': comment_dates.pop(0) if comment_dates else 'N/A',  # comment_dates에서 첫 번째 요소 제거\n",
    "            'sub_comment_count': comment_sub_counts.pop(0) if comment_sub_counts else 0  # sub_comment_counts에서 첫 번째 요소 제거\n",
    "        })\n",
    "    \n",
    "    # 데이터프레임으로 변환\n",
    "    table = table.append(pd.DataFrame(results), ignore_index=True)\n",
    "    print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 크롤링\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "urls = pd.read_excel('유튜브영상리스트.xlsx', header=0)\n",
    "\n",
    "for column in range(len(urls.columns)):\n",
    "    selected_urls = urls.iloc[:, column]\n",
    "    \n",
    "    for url in selected_urls:\n",
    "        if \"shorts\" in url:\n",
    "            short_video(url)\n",
    "        else:\n",
    "            general_video(url)\n",
    "        \n",
    "print(\"댓글 데이터프레임:\")\n",
    "print(table)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개 더 크롤링\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "urls = pd.read_excel('유튜브영상리스트.xlsx', header=0)\n",
    "\n",
    "for column in range(8,10):\n",
    "    selected_urls = urls.iloc[:, column]\n",
    "\n",
    "    for url in selected_urls:\n",
    "        if \"shorts\" in url:\n",
    "            short_video(url)\n",
    "        else:\n",
    "            general_video(url)\n",
    "\n",
    "print(\"댓글 데이터프레임:\")\n",
    "print(table)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어버전 따로 저장\n",
    "table.to_excel('ENG유튜브영상크롤링DF.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기존 테이블과 합치기\n",
    "pre_table = pd.read_excel('유튜브영상크롤링DF.xlsx', header=0)\n",
    "combined_table = pd.concat([pre_table, table], ignore_index=True)\n",
    "\n",
    "print(combined_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#엑셀화\n",
    "combined_table.to_excel('병합유튜브영상크롤링DF.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('병합유튜브영상크롤링DF.xlsx')\n",
    "print(df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###패키지 다운로드\n",
    "import nltk\n",
    "\n",
    "# 필수 리소스 목록\n",
    "resources = ['wordnet', 'stopwords', 'punkt', 'omw-1.4']\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        # 리소스가 이미 있는지 확인 (실제 로드를 시도하지 않고 경로만 확인)\n",
    "        nltk.data.find(f'corpora/{resource}' if resource in ['wordnet', 'stopwords', 'omw-1.4'] else f'tokenizers/{resource}')\n",
    "        print(f\"Resource '{resource}' already downloaded.\")\n",
    "    except LookupError:\n",
    "        print(f\"Resource '{resource}' not found. Downloading...\")\n",
    "        nltk.download(resource, quiet=True) # quiet=True는 다운로드 과정을 간결하게 표시\n",
    "        print(f\"Resource '{resource}' downloaded.\")\n",
    "    except Exception as e:\n",
    "        # nltk.downloader.DownloadFailure 같은 다른 예외 처리\n",
    "        print(f\"Error checking/downloading resource '{resource}': {e}\")\n",
    "        print(f\"Attempting download for '{resource}' again.\")\n",
    "        try:\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"Resource '{resource}' downloaded successfully after retry.\")\n",
    "        except Exception as e_retry:\n",
    "            print(f\"Failed to download '{resource}' even after retry: {e_retry}\")\n",
    "            print(f\"Please try running 'nltk.download(\\\"{resource}\\\")' manually in a Python console.\")\n",
    "\n",
    "print(\"NLTK resource check/download process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed 설정\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# 객체 생성\n",
    "okt = Okt()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stopword 세트\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "korean_stop_words = set([\n",
    "    '이', '그', '저', '것', '수', '들', '좀', '더', '등', '때문', '그리고',\n",
    "    '그러나', '하지만', '또는', '즉', '즉시', '같다', '다시',\n",
    "    '많이', '많은', '아주', '매우', '조금', '거의', '이제', '우리', '너무',\n",
    "    '한다', '했다', '하게', '하려', '해서', '하며',\n",
    "    '하는', '하자', '하기', '하면', '하니까', '하네', '함', '됨', '됨니다',\n",
    "    '되다', '되어', '되어서', '되면', '됩니다', '자신', '중', '내', '합시다', \n",
    "    '니', '가요', '어요', '네요', '군요', '입니다',\n",
    "    '가', '은', '는', '의', '에', '와', '과', '을', '를', '로', '자', '관련', '\\n', '/n'\n",
    "    '때', '도', '에서', '하다', '있다', '없다', '대해', '않다', '.', '더욱', '끼리', '뭔가',\n",
    "    '요', '님', '죠', '네', '어', '께', '데', '딘', '돗', '게', '께서', '합니다',\n",
    "    '진짜', '정말', '그냥', '뭐', '하나', '처럼', '보기', '까지', '거', '건', '되', '듯',\n",
    "    '한테', '라니', '그래도', '약간', '또한', '이런', '저런', 'ㅋ', '이다'\n",
    "])\n",
    "\n",
    "# 전처리 함수\n",
    "def clean_text(comment):\n",
    "    if pd.isna(comment) or len(comment.strip()) == 0:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        language = detect(comment)\n",
    "    except LangDetectException:\n",
    "        return ''\n",
    "    \n",
    "    # 문장 -> 단어 토큰화, 특수문자 제거, 소문자화\n",
    "    def preprocess_word(word):\n",
    "        word = word.lower()\n",
    "        word = re.sub(r'[^\\w\\s]', '', word)  # 특수문자 제거\n",
    "        return word\n",
    "\n",
    "    if language == 'en':\n",
    "        sentences = sent_tokenize(comment)\n",
    "        words = []\n",
    "        for sent in sentences:\n",
    "            tokens = word_tokenize(sent)\n",
    "            for word in tokens:\n",
    "                word = preprocess_word(word)\n",
    "                if len(word) < 3 or word in english_stop_words:\n",
    "                    continue\n",
    "                # lemmatize (동사, 명사 우선)\n",
    "                lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "                lemma = lemmatizer.lemmatize(lemma, pos='n')\n",
    "                stemmed = stemmer.stem(lemma)\n",
    "                words.append(stemmed)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    elif language == 'ko':\n",
    "        desired_pos = ['Noun', 'Adjective']\n",
    "        tokens = okt.pos(comment, norm=True, stem=True) # 품사 태깅 및 정규화, 어간 추출\n",
    "        words = []\n",
    "        for word, pos in tokens:\n",
    "            if pos in desired_pos: # 원하는 품사만 필터링\n",
    "                word = preprocess_word(word)\n",
    "                if len(word) < 2 or word in korean_stop_words:\n",
    "                    continue\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 적용\n",
    "df['cleaned_text'] = df['comment'].apply(clean_text)\n",
    "\n",
    "# 결과 확인\n",
    "print(df[['comment', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 결과 엑셀로 간단 정리\n",
    "test_df = df[['comment', 'cleaned_text']]\n",
    "\n",
    "test_df.to_excel('댓글전처리결과.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('댓글전처리결과.xlsx')\n",
    "df2 = pd.read_excel('병합유튜브영상크롤링DF.xlsx')\n",
    "\n",
    "df2['cleaned_text'] = df['cleaned_text']\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 기반 LDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "df1 = pd.read_excel('댓글전처리결과.xlsx')\n",
    "df = pd.read_excel('병합유튜브영상크롤링DF.xlsx')\n",
    "\n",
    "df['cleaned_text'] = df1['cleaned_text']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14개까지 했을 때 \n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "all_results = []\n",
    "summary_results = []\n",
    "\n",
    "# 영상(title)별로 그룹화\n",
    "for vid, group in df.groupby('title'):\n",
    "    print(f\"\\n영상 제목: {vid} ===\")\n",
    "\n",
    "    # 전처리\n",
    "    group = group[group['cleaned_text'].notnull()]\n",
    "    group = group[group['cleaned_text'].apply(lambda x: isinstance(x, str))]\n",
    "    documents = group['cleaned_text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    coherence_scores = []\n",
    "    perplexity_scores = []\n",
    "    topic_nums = list(range(2, 14))\n",
    "\n",
    "    best_coherence = -1\n",
    "    best_model = None\n",
    "    best_num_topics = 0\n",
    "    best_perplexity = None\n",
    "\n",
    "    for num_topics in topic_nums:\n",
    "        lda_model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                             num_topics=num_topics, passes=5, random_state=42)\n",
    "\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=documents,\n",
    "                                         dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        perplexity = lda_model.log_perplexity(corpus)\n",
    "\n",
    "        coherence_scores.append(coherence_score)\n",
    "        perplexity_scores.append(perplexity)\n",
    "\n",
    "        all_results.append({\n",
    "            'title': vid,\n",
    "            'num_topics': num_topics,\n",
    "            'coherence': coherence_score,\n",
    "            'perplexity': perplexity\n",
    "        })\n",
    "        \n",
    "        print(f\"토픽 {num_topics}개 - Coherence: {coherence_score:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "        if coherence_score > best_coherence:\n",
    "            best_coherence = coherence_score\n",
    "            best_model = lda_model\n",
    "            best_num_topics = num_topics\n",
    "            best_perplexity = perplexity\n",
    "\n",
    "    # 그래프 1: Coherence\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(topic_nums, coherence_scores, marker='o', color='blue')\n",
    "    plt.title(f\"[Coherence] {vid}\")\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence Score\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 그래프 2: Perplexity\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(topic_nums, perplexity_scores, marker='s', color='red')\n",
    "    plt.title(f\"[Perplexity] {vid}\")\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Perplexity (log)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 최적 모델 결과 출력 및 저장\n",
    "    print(f\"\\n최적 토픽 수: {best_num_topics} (Coherence: {best_coherence:.4f}, Perplexity: {best_perplexity:.4f})\")\n",
    "    topic_list = []\n",
    "    for idx, topic in best_model.print_topics(num_words=5):\n",
    "        print(f\"토픽 {idx + 1}: {topic}\")\n",
    "        topic_list.append(f\"토픽 {idx + 1}: {topic}\")\n",
    "\n",
    "    summary_results.append({\n",
    "        'title': vid,\n",
    "        'best_num_topics': best_num_topics,\n",
    "        'best_coherence': best_coherence,\n",
    "        'best_perplexity': best_perplexity,\n",
    "        'topics': \"\\n\".join(topic_list)\n",
    "    })\n",
    "\n",
    "# 엑셀 저장\n",
    "result_df = pd.DataFrame(all_results)\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "\n",
    "with pd.ExcelWriter(\"토픽분석_결과정리.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    result_df.to_excel(writer, sheet_name=\"전체 토픽별 기록\", index=False)\n",
    "    summary_df.to_excel(writer, sheet_name=\"최적 토픽 요약\", index=False)\n",
    "\n",
    "print(\"결과 기록 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "## 9개로 토픽 고정 ##\n",
    "##################\n",
    "\n",
    "import seaborn as sns\n",
    "from itertools import islice\n",
    "\n",
    "# 실험할 하이퍼파라미터 조합\n",
    "passes_list = [10, 15, 20]\n",
    "iterations_list = [50, 100, 150]\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "all_results = []\n",
    "summary_results = []\n",
    "\n",
    "# 영상(title)별로 그룹화 후 첫 10개만 처리\n",
    "for vid, group in islice(df.groupby('title'), 10):\n",
    "    print(f\"\\n영상 제목: {vid}\")\n",
    "\n",
    "    # 전처리\n",
    "    group = group[group['cleaned_text'].notnull()]\n",
    "    group = group[group['cleaned_text'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "    # 여기에 _x000d_ 문자 제거 추가\n",
    "    group['cleaned_text'] = group['cleaned_text'].str.replace('_x000d_', '')\n",
    "    group['cleaned_text'] = group['cleaned_text'].str.strip() # 공백 제거\n",
    "\n",
    "    documents = group['cleaned_text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "    # 빈 문서 필터링 (documents가 비어있는 경우 오류 방지)\n",
    "    documents = [doc for doc in documents if doc]\n",
    "    if not documents:\n",
    "        print(f\"[{vid}] 처리할 문서가 없습니다. 다음 영상으로 넘어갑니다.\")\n",
    "        continue\n",
    "\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    num_topics = 9 # 토픽 수 고정\n",
    "    best_overall_score = -1\n",
    "    best_overall_model = None\n",
    "    best_params = {}\n",
    "\n",
    "    for passes in passes_list:\n",
    "        for iterations in iterations_list:\n",
    "            lda_model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                                    num_topics=num_topics, passes=passes,\n",
    "                                    iterations=iterations, random_state=42)\n",
    "\n",
    "            coherence_model = CoherenceModel(model=lda_model, texts=documents,\n",
    "                                                dictionary=dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "            perplexity = lda_model.log_perplexity(corpus)\n",
    "\n",
    "            all_results.append({\n",
    "                'title': vid,\n",
    "                'num_topics': num_topics,\n",
    "                'passes': passes,\n",
    "                'iterations': iterations,\n",
    "                'coherence': coherence_score,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "\n",
    "            print(f\"[{vid}] 토픽 {num_topics}개 | passes={passes}, iterations={iterations} -> \"\n",
    "                    f\"Coherence: {coherence_score:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "            if coherence_score > best_overall_score:\n",
    "                best_overall_score = coherence_score\n",
    "                best_overall_model = lda_model\n",
    "                best_params = {\n",
    "                    'num_topics': num_topics,\n",
    "                    'passes': passes,\n",
    "                    'iterations': iterations,\n",
    "                    'coherence': coherence_score,\n",
    "                    'perplexity': perplexity\n",
    "                }\n",
    "\n",
    "    # 최적 모델 결과 요약\n",
    "    print(f\"\\n최적 설정 ({vid}):\")\n",
    "    print(f\"토픽 수: {best_params['num_topics']}, passes: {best_params['passes']}, \"\n",
    "          f\"iterations: {best_params['iterations']}\")\n",
    "    print(f\"Coherence: {best_params['coherence']:.4f}, Perplexity: {best_params['perplexity']:.4f}\")\n",
    "\n",
    "    topic_list = []\n",
    "    # best_overall_model이 None이 아닐 경우에만 print_topics 호출\n",
    "    if best_overall_model:\n",
    "        for idx, topic in best_overall_model.print_topics(num_words=5):\n",
    "            print(f\"토픽 {idx + 1}: {topic}\")\n",
    "            topic_list.append(f\"토픽 {idx + 1}: {topic}\")\n",
    "    else:\n",
    "        print(f\"[{vid}] 최적 모델이 생성되지 않았습니다.\")\n",
    "\n",
    "\n",
    "    summary_results.append({\n",
    "        'title': vid,\n",
    "        **best_params,\n",
    "        'topics': \"\\n\".join(topic_list)\n",
    "    })\n",
    "\n",
    "    # 시각화를 위한 DataFrame 생성\n",
    "    # 해당 vid에 대한 결과만 필터링하여 heatmap_data 만들기\n",
    "    vid_results = [r for r in all_results if r['title'] == vid and r['num_topics'] == num_topics]\n",
    "\n",
    "    heatmap_data_coherence = pd.DataFrame(index=passes_list, columns=iterations_list, dtype=float)\n",
    "    heatmap_data_perplexity = pd.DataFrame(index=passes_list, columns=iterations_list, dtype=float)\n",
    "\n",
    "    for record in vid_results:\n",
    "        heatmap_data_coherence.loc[record['passes'], record['iterations']] = record['coherence']\n",
    "        heatmap_data_perplexity.loc[record['passes'], record['iterations']] = record['perplexity']\n",
    "\n",
    "    # 그래프 1: Coherence 히트맵\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(heatmap_data_coherence, annot=True, fmt=\".4f\", cmap=\"viridis\", linewidths=.5)\n",
    "    plt.title(f\"Coherence Score Heatmap for {vid} (Num Topics: {num_topics})\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Passes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 그래프 2: Perplexity 히트맵\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(heatmap_data_perplexity, annot=True, fmt=\".4f\", cmap=\"magma_r\", linewidths=.5) # perplexity는 낮을수록 좋으므로 _r (reverse) 사용\n",
    "    plt.title(f\"Perplexity Heatmap for {vid} (Num Topics: {num_topics})\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Passes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 최적 모델 결과 출력 (여기서는 best_params 사용)\n",
    "    print(f\"\\n최적 반복 수: (Coherence: {best_params['coherence']:.4f}, Perplexity: {best_params['perplexity']:.4f})\")\n",
    "    print(f\"최적 Passes: {best_params['passes']}, 최적 Iterations: {best_params['iterations']}\")\n",
    "    print(\"최적 모델의 토픽:\")\n",
    "    topic_list = []\n",
    "    if best_overall_model: # best_overall_model이 None이 아닐 경우에만 print_topics 호출\n",
    "        for idx, topic in best_overall_model.print_topics(num_words=5):\n",
    "            print(f\"토픽 {idx + 1}: {topic}\")\n",
    "            topic_list.append(f\"토픽 {idx + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### passes=20, iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from itertools import islice\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "all_results = []\n",
    "summary_results = []\n",
    "\n",
    "# 토픽 수 탐색 범위\n",
    "num_topics_list = list(range(2, 16)) # 2부터 15까지\n",
    "\n",
    "# 영상(title)별로 그룹화\n",
    "for vid, group in df.groupby('title'):\n",
    "    print(f\"\\n영상 제목: {vid}\")\n",
    "\n",
    "    # 전처리\n",
    "    group = group[group['cleaned_text'].notnull()]\n",
    "    group['cleaned_text'] = group['cleaned_text'].str.replace('_x000d_', '')\n",
    "    documents = group['cleaned_text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "    # 빈 문서 필터링 (documents가 비어있는 경우 오류 방지)\n",
    "    documents = [doc for doc in documents if doc]\n",
    "    if not documents:\n",
    "        print(f\"[{vid}] 처리할 문서가 없습니다. 다음 영상으로 넘어갑니다.\")\n",
    "        continue\n",
    "    \n",
    "    # dictionary와 corpus는 각 영상/문서에 대해 새로 생성\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    \n",
    "    # 너무 적은 단어는 필터링 (선택 사항, 모델 품질 향상에 도움)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5) # 5개 미만 문서에 나타난 단어, 50% 이상 문서에 나타난 단어 제거\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    # 코퍼스가 비어있는 경우 (단어 필터링 후) 오류 방지\n",
    "    if not corpus:\n",
    "        print(f\"[{vid}] 필터링 후 코퍼스가 비어있습니다. 다음 영상으로 넘어갑니다.\")\n",
    "        continue\n",
    "\n",
    "    coherence_scores = []\n",
    "    perplexity_scores = []\n",
    "    \n",
    "    best_coherence = -1 # 가장 높은 Coherence Score\n",
    "    best_model = None # 가장 높은 Coherence Score를 달성한 모델\n",
    "    best_num_topics_for_vid = 0 # 해당 영상의 최적 토픽 수\n",
    "    best_perplexity_for_vid = None # 해당 영상의 최적 Perplexity\n",
    "\n",
    "    # num_topics_list (2~14개)를 순회하며 모델 학습 및 평가\n",
    "    for num_topic_candidate in num_topics_list:\n",
    "        try:\n",
    "            lda_model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                                 num_topics=num_topic_candidate,\n",
    "                                 passes=20,\n",
    "                                 iterations=100, \n",
    "                                 random_state=42)\n",
    "\n",
    "            coherence_model = CoherenceModel(model=lda_model, texts=documents,\n",
    "                                             dictionary=dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "            perplexity = lda_model.log_perplexity(corpus)\n",
    "\n",
    "            coherence_scores.append(coherence_score)\n",
    "            perplexity_scores.append(perplexity)\n",
    "\n",
    "            # 각 num_topic_candidate에 대한 결과 저장\n",
    "            all_results.append({\n",
    "                'title': vid,\n",
    "                'num_topics': num_topic_candidate,\n",
    "                'coherence': coherence_score,\n",
    "                'perplexity': perplexity\n",
    "            })\n",
    "            \n",
    "            print(f\"[{vid}] 토픽 {num_topic_candidate}개 - Coherence: {coherence_score:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "\n",
    "            # 현재 영상의 최적 토픽 수 찾기 (Coherence 기반)\n",
    "            if coherence_score > best_coherence:\n",
    "                best_coherence = coherence_score\n",
    "                best_model = lda_model\n",
    "                best_num_topics_for_vid = num_topic_candidate\n",
    "                best_perplexity_for_vid = perplexity\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"[{vid}] 토픽 {num_topic_candidate}개 모델 학습 중 오류 발생: {e}\")\n",
    "            # 오류 발생 시 해당 토픽 수에 대한 결과는 추가하지 않음\n",
    "            coherence_scores.append(0.0) # 또는 NaN\n",
    "            perplexity_scores.append(0.0) # 또는 NaN\n",
    "\n",
    "\n",
    "    # 각 영상별 그래프 1: Coherence\n",
    "    if coherence_scores: # 데이터가 있을 때만 그래프 그리기\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(num_topics_list[:len(coherence_scores)], coherence_scores, marker='o', color='blue')\n",
    "        plt.title(f\"[Coherence] {vid}\")\n",
    "        plt.xlabel(\"Number of Topics\")\n",
    "        plt.ylabel(\"Coherence Score\")\n",
    "        plt.grid(True)\n",
    "        plt.xticks(num_topics_list[:len(coherence_scores)]) # x축 레이블 설정\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 각 영상별 그래프 2: Perplexity\n",
    "    if perplexity_scores: # 데이터가 있을 때만 그래프 그리기\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(num_topics_list[:len(perplexity_scores)], perplexity_scores, marker='s', color='red')\n",
    "        plt.title(f\"[Perplexity] {vid}\")\n",
    "        plt.xlabel(\"Number of Topics\")\n",
    "        plt.ylabel(\"Perplexity (log)\")\n",
    "        plt.grid(True)\n",
    "        plt.xticks(num_topics_list[:len(perplexity_scores)]) # x축 레이블 설정\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 최적 모델 결과 출력 및 저장\n",
    "    if best_model: # best_model이 존재하는 경우에만\n",
    "        print(f\"\\n최적 토픽 수 ({vid}): {best_num_topics_for_vid} \"\n",
    "              f\"(Coherence: {best_coherence:.4f}, Perplexity: {best_perplexity_for_vid:.4f})\")\n",
    "        topic_list = []\n",
    "        for idx, topic in best_model.print_topics(num_words=5):\n",
    "            print(f\"토픽 {idx + 1}: {topic}\")\n",
    "            topic_list.append(f\"토픽 {idx + 1}: {topic}\")\n",
    "\n",
    "        summary_results.append({\n",
    "            'title': vid,\n",
    "            'best_num_topics': best_num_topics_for_vid,\n",
    "            'best_coherence': best_coherence,\n",
    "            'best_perplexity': best_perplexity_for_vid,\n",
    "            'topics': \"\\n\".join(topic_list)\n",
    "        })\n",
    "    else:\n",
    "        print(f\"\\n[{vid}] 최적 모델을 찾을 수 없거나 데이터가 부족합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 저장\n",
    "result_df = pd.DataFrame(all_results)\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "\n",
    "file_path = \"토픽 분석 결과.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine=\"xlsxwriter\") as writer:\n",
    "    result_df.to_excel(writer, sheet_name=\"전체 토픽별 기록\", index=False)\n",
    "    summary_df.to_excel(writer, sheet_name=\"최적 토픽 요약\", index=False)\n",
    "\n",
    "print(f\"\\n결과 기록 저장 완료: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "excel_file_path = \"토픽 분석 결과.xlsx\"\n",
    "sheet_name = \"최적 토픽 요약\"\n",
    "\n",
    "df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_col = 'best_coherence'\n",
    "perplexity_col = 'best_perplexity'\n",
    "\n",
    "for row in df:\n",
    "    #coherence\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(coherence_scores, marker='o', color='blue')\n",
    "    plt.title(f\"[Coherence] \", row[0])\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence Score\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(num_topics_list[:len(coherence_scores)]) # x축 레이블 설정\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #perplexity\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(num_topics_list[:len(perplexity_scores)], perplexity_scores, marker='s', color='red')\n",
    "    plt.title(f\"[Perplexity] \", row[0])\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Perplexity (log)\")\n",
    "    plt.grid(True)\n",
    "    plt.xticks(num_topics_list[:len(perplexity_scores)]) # x축 레이블 설정\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
